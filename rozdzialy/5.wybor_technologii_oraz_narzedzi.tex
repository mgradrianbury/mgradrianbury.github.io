\chapter{Wybór technologii oraz narzędzi}


\section{Wykrywanie twarzy na zdjęciu}

Wykrywanie twarzy to pierwszy krok w zaprojektowanym systemie.
Z powodu, że jest on umieszczony na samym początku, proces ten musi być maksymalnie skuteczny.
Jeżeli na zdjęciu nie zostanie wykryta twarz, to cały proces zakończy się z negatywnym rezultatem.
\textbf{Twarzy nie będzie można zidentyfikować, jeżeli nie zostanie ona znaleziona.}
Oznacza to, że twarze muszą być wykrywane z różnymi orientacjami, kątami,
poziomem oświetlenia, makijażem, kapeluszami, zarostem, fryzurami, okularami, wiekiem i tak dalej.
Niestety twarz ludzka jest obiektem dynamicznym i ma duży stopień zmienności w wyglądzie,
co sprawia, że wykrywanie twarzy jest trudnym problemem w widzeniu komputerowym~\cite{HJELMAS2001236}.

W artykule z 2016 roku zatytułowanym
``Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks''~\cite{zhang2016joint}
została zaproponowana wielozadaniowa kaskadowa konwolucyjna sieć neuronowa
(MTCNN\footnote{Skrót z języka angielskiego od Multi-Task Cascaded Convolutional Neural Networks })
do wykrywania twarzy.
Sieć ta zyskała duża popularność, ponieważ osiągnęła wówczas najlepszy wyniki
w wykrywaniu twarzy na zdjęciach dla wybranych zbiorów danych.
Kolejną zaletą zaproponowanej sieci neuronowej jest to, że jest w stanie
rozpoznać punkty orientacyjne twarzy, takie jak oczy, usta i nos.
Wyniki przeprowadzonych eksperymentów w artykule zostały przedstawione na rysunku~\ref{fig:mtcnn_wyniki}.

\begin{figure}[P]
    \centering
    \includegraphics[width=1\textwidth]{images/mtcnn-wyniki}
    \caption{
        Wyniki przeprowadzonych eksperymentów. Słowo ``Ours'' oznacza sieć MTCNN.
        (a) wyniki z testów na bazie FDDB.
        (b-d) wyniki z testów dla trzech kolejnych zbiorów na bazy Wider Face.
        (e) Wyniki z testów dla wyrównania twarzy na bazie AFLW.
    }
    \bibsource{\cite{zhang2016joint}}
    \label{fig:mtcnn_wyniki}
\end{figure}


W internecie znajduje się spora liczba implementacji MTCNN~\cite{mtznn_all_impls}.
Spośród dostępnych została wybrana sieć napisana przez \textit{Iván de Paz Centeno}
i udostępniona na portalu GitHub\footnote{GitHub - dostawcą platformy internetowej do tworzenia
oprogramowania i kontroli wersji za pomocą narzędzia Git udostępniania
pod adresem \url{https://github.com}. } (\url{https://github.com/ipazc/mtcnn}) na zasadach licencji MIT\footnote{Licencja MIT daje użytkownikom
nieograniczone prawo do używania, kopiowania, modyfikowania i rozpowszechniania (w tym sprzedaży)
    oryginalnego lub zmodyfikowanego programu w postaci binarnej lub źródłowej.
    Jedynym wymaganiem jest, by we wszystkich wersjach zachowano warunki licencyjne i informacje o autorze.
}~\cite{ipazc/mtcnn}.
Główną zaletą wybranej implementacji jest łatwość użycia.
Sieć została napisana w języku programowania python w bibliotece TensorFlow
i całość udostępniona jako pakiet z możliwością instalacji
przez PIP\footnote{PIP - narzędzie do instalowania pakietów python}.


\section{FaceNet}

FaceNet to architektura, oparta na głębokiej sieci neuronowej, która bezpośrednio uczy się mapowania
z obrazów twarzy do zwartej przestrzeni euklidesowej,
w której odległości bezpośrednio odpowiadają mierze podobieństwa twarzy~\cite{schroff2015facenet}.
Wynikiem działania sieci jest wektor składający się z elementów charakteryzujący daną twarz.
Długość wektora jest zależna od konkretnej implementacji.
Po utworzeniu przestrzeni euklidesowej na bazie dostępnych zdjęć twarzy,
zadania polegające na identyfikacji, weryfikacji czy grupowaniu
sprowadzają się do pomiaru odległości pomiędzy poszczególnymi wektorami cech~\cite{schroff2015facenet}.
Wizualizacja procesu mapowania zdjęcia do przestrzeni
euklidesowej została przedstawiona na rysunku~\ref{fig:facenet_zastosowanie}.

\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{images/facenet_euc}
    \caption{
        Proces umieszczania wektorów cech w przestrzeni euklidesowej.
        Zdjęcia przedstawiające tę samą osobę znajdą się bliżej siebie względem zdjęć innych osób.
        Literą ``A'' zostały oznaczone zdjęcia przedstawiające tę samą osobę.
    }
    \customsource
    \label{fig:facenet_zastosowanie}
\end{figure}

\subsection{Zasada działania}

FaceNet pobiera obraz twarzy osoby jako dane wejściowe i wyprowadza wektor,
który reprezentuje najważniejsze cechy twarzy (wektor cech).
Operacje ta, jest pewnego rodzaju kompresją, której zadaniem jest zmniejszenie ilości informacji
na temat danej rzeczy (w tym wypadku zdjęcia twarzy) bez utraty kluczowych informacji.
Informacje, jakie są pobierane ze zdjęcia, nie są z góry zdefiniowane.
Za odpowiedni dobór parametrów jest odpowiedzialna głęboka sieć neuronowa, która traktowana jest
jako czarna skrzynka, czyli miejsce, gdzie znane są tylko dane wejściowe oraz dane wyjściowe.
Oznacza to, że nie są znane parametry, które są pobierane ze zdjęcia,
a wartości w wektorze cech generowane na wyjściu są trudne, o ile w ogóle możliwe do zinterpretowania.

Oprócz samej sieci neuronowej FaceNet składa się również z innych warstw: normalizacji L2 oraz
funkcji strat \textit{triplet loss} (rysunek~\ref{fig:facenet_arch}).
Normalizacja L2 polega na takim zmodyfikowaniu wartości otrzymanego zbioru, aby w każdym wierszu
suma kwadratów była zawsze równa $1$,
natomiast zadaniem funkcji \textit{triplet loss} jest, w procesie trenowania sieci,
minimalizowanie odległości pomiędzy prawdziwymi
wyrażeniami oraz maksymalizowanie dla fałszywych~\cite{chechik2010large}.
Innymi słowy, celem jest, aby odległości między wektorami tych samych osób były jak najmniejsze,
a odległości pomiędzy wektorami różnych osób --- maksymalne.
Wzór matematyczny został przedstawiony równaniem~\ref{eq:triplet_loss},
natomiast zasada działania w sposób graficzny przedstawiona na rysunku~\ref{fig:triplet_loss}.


\begin{equation}
    L = \sum_{i}^{N} [  \| f(x_i^a) - f(x_i^p) \| ^2_2 - \| f(x_i^a) - f(x_i^n) \| ^2_2 + \alpha ] _+
    \label{eq:triplet_loss}
\end{equation}

gdzie:

$x_i^a$ - wektor zdjęcia wzorcowego

$x_i^p$ - wektor zdjęcia ``prawdziwego'' względem wzorca

$x_i^n$ - wektor zdjęcia ``fałszywego'' względem wzorca

$\alpha$ - margines odległości pomiędzy wektorami

\pagebreak

\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{images/facenet_arch}
    \caption{ Architektura FaceNet z podziałem na poszczególne warstwy }
    \customsource
    \label{fig:facenet_arch}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{images/triplet_loss}
    \caption{ Uproszczony schemat obrazujący zasadę działania funkcji strat \textit{triplet loss} }
    \customsource
    \label{fig:triplet_loss}
\end{figure}


Całość procesu uczenia się sieci FaceNet, można w uproszczeniu podsumować w następujących krokach:

\begin{enumerate}
    \item zdefiniowanie parametrów początkowych sieci,
    \item wygenerowanie wektorów cech twarzy,
    \item użycie funkcji strat \textit{triplet loss},
    \item dostosowanie się parametrów sieci tak, aby przykład pozytywny był bliżej zdjęcia wzorcowego, niż przykład negatywny,
    \item powrót do kroku drugiego tak długo, aż zwracane wektory osiągną zadaną dokładność
\end{enumerate}

\subsection{Wpływ wielkości zdjęcia na działanie sieci}

W pracy pod tytułem \textit{Facenet: A unified embedding for face recognition and clustering}~\cite{schroff2015facenet}
została wytrenowana sieć o architekturze FaceNet na zdjęciach w formacie JPEG o rozmiarach 220 na 220 pikseli.
Podczas testowania wytrenowanej sieci, po dostarczeniu zdjęć o mniejszych rozmiarach (względem rozmiarów
zdjęć, na których sieć była trenowana) okazało się, że sieć ta jest również bardzo skuteczna.
Sieć miała akceptowalną skuteczność nawet po dostarczeniu zdjęć o rozmiarach 80 na 80 pikseli.
Trenowanie sieci na zdjęciach o mniejszej rozdzielczości mogłoby jeszcze poprawić ten wynik.
Dokładne informacje na temat wpływu liczby pikseli na
trafność sieci zostały przedstawione w tabeli~\ref{tab:quality_pixels_to_rate}

\begin{table}[]
    \caption{Wpływ jakości zdjęcia oraz liczby pikseli na trafność sieci FaceNet. Źródło: \cite{schroff2015facenet}}
    \label{tab:quality_pixels_to_rate}
    \begin{minipage}{.5\linewidth}
        \centering
        \begin{tabular}{|l|l|}
            \hline
            Liczba pikseli                & Trafność \\ \hline
            1 600 \hspace{15px} (40x40)   & 37.8\%   \\ \hline
            6 400 \hspace{15px}     (80x80)    & 79.5\%   \\ \hline
            14 400 \hspace{10px}  (120x120) & 84.5\%   \\ \hline
            25 600 \hspace{10px}  (160x160) & 85.7\%   \\ \hline
            65 536 \hspace{10px}  (256x256) & 86.4\%   \\ \hline
        \end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
        \centering
        \begin{tabular}{|l|l|}
            \hline
            Jakość JPEG & Trafność \\ \hline
            10          & 67.3\%   \\ \hline
            20          & 81.4\%   \\ \hline
            30          & 83.9\%   \\ \hline
            50          & 85.5\%   \\ \hline
            70          & 86.1\%   \\ \hline
            90          & 86.5\%   \\ \hline
        \end{tabular}
    \end{minipage}
\end{table}

\subsection{Użycie wytrenowanej sieci}

W projektowanym systemie zostanie użyty wytrenowany model \textit{Keras FaceNet}~\cite{taniai-2018},
który został wyszkolony na bazie zdjęć dostarczonych przez MS-Celeb-1M~\cite{microsoft-2020-celeb1m}
i udostępniony przez \textit{Hiroki Taniai}~\cite{taniai-no-date}.
Dostarczane zdjęcia twarzy powinny być kolorowe, w formacie RGB oraz
o rozmiarach 160 pikseli na 160 pikseli~\cite{brownlee-2019}.


\section{Kadrowanie i standaryzacja}

Do przetwarzania plików graficznych zostanie wykorzystana biblioteka \textit{PILLOW} napisana w języku python.
Biblioteka wspiera wiele formatów graficznych,
w tym te najpopularniejsze jak \textit{PNG}, \textit{GIF}, \textit{JPEG} oraz \textif{BMP}~\cite{pillow_doc},
dzięki czemu nie będzie wymagane, aby użytkownik sam konwertował plików graficznych do odpowiedniego formatu.
Biblioteka \textit{PILLOW} zostanie również wykorzystana do wycinania zdjęcia twarzy oraz standaryzacji
i normalizacji już wykadrowanego zdjęcia.


\section{Klasyfikacja}

Ostatnim krokiem w systemie jest dokonanie klasyfikacji wygenerowanego wektora cech,
czyli określenie, do jakiej grupy należy wektor na podstawie próbek uczących.
Innymi słowy, etap ten polega na zwróceniu informacji, do kogo najbardziej pasuje przesłane zdjęcie,
opierając się na fotografiach dostępnych w systemie.

Jednym z szeroko stosowanych algorytmów klasyfikacji jest
\textbf{maszyna wektorów nośnych} (ang. support vector machine — SVM).
Klasyfikator ten konstruuje hiperpłaszczyznę lub ich zbiór w przestrzeni wielowymiarowej,
na podstawie otrzymanych próbek, która oddziela poszczególne klasy.
Optymalizowanie modelu SVM polega na maksymalizacji marginesu pomiędzy płaszczyzną a poszczególnymi klasami,
ponieważ na ogół nim większy margines, tym mniejszy jest błąd generalizacji~\cite{hastie2009elements}.
Koncepcja klasyfikatora została zaprezentowana na rysunku~\ref{fig:svm}.


\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{images/smv}
    \caption{
        Model maszyny wektorów nośnych.
        Po lewej możliwe położenia hiperpłaszczyzny, po prawej zoptymalizowany model wraz z objaśnieniami.
    }
    \bibsource{\cite{raschka2017python}}
    \label{fig:svm}
\end{figure}

Jeśli dane uczące są liniowo separowalne, to można wybrać dwie równoległe hiperpłaszczyzny,
które oddzielają dwie klasy danych, tak aby odległość między nimi była maksymalna.
Obszar ograniczony przez te dwie hiperpłaszczyzny nazywany jest ``marginesem'',
a hiperpłaszczyzna maksymalnego marginesu to hiperpłaszczyzna leżąca w połowie odległości między nimi.
Problem optymalizacji algorytmu został przedstawiony równaniem~\ref{eq:hard_margin}.
Lewą stroną tego równania jest odległość pomiędzy hiperpłaszczyzną ``pozytywną'' a ``negatywną'',
zatem, w celu maksymalizacji marginesu, należy zmaksymalizować $\frac{2}{||w||}$ lub
minimalizować odwrotność wyrażenia, czyli $\frac{1}{2}||w||^2$.

\begin{equation}
    \begin{aligned}
        & \frac{ w^T (x_{poz} - x_{neg}) }{ ||w|| } = \frac{2}{||w||} \\
        \text{gdzie:} \\
        w & \text{ - wektor normalny do granicy decyzyjności}
        \\
        x_{poz} & \text{ - ``pozytywny'' wektor nośny}
        \\
        x_{neg} & \text{ - ``negatywny'' wektor nośny}
    \end{aligned}
    \label{eq:hard_margin}
\end{equation}
\bigskip

W przypadku danych liniowo separowalnych równanie~\ref{eq:hard_margin} musi
spełniać warunki przedstawione w równaniu~\ref{eq:hard_margin_class}.
Równania te oznaczają, że wszystkie negatywne próbki powinny wylądować po stronie
negatywnej hiperpłaszczyzny, a wszystkie próbki pozytywne po stronie hiperpłaszczyzny pozytywnej.

\begin{equation}
    \begin{aligned}
        w_0 +w^{T}x^{(i)} &\geq 1 && \text{ jeśli } y^{(i)} =1
        \\
        w_0 +w^{T}x^{(i)} &\leq -1 && \text{ jeśli } y^{(i)} =-1
        \\
        \text{dla } i&= 1\dots N
        \\
        \text{ gdzie: } \\
        N & \text{  - liczba próbek}
        \\
        w_0 +w^{T}x^{(i)} & \text{  - odległość od granicy decyzyjności}
    \end{aligned}
    \label{eq:hard_margin_class}
\end{equation}

\bigskip

W przypadku, gdy dane nie są liniowo separowalne (rysunek~\ref{fig:sofm_margin}) wprowadza się dodatkową zmienną $\xi$.
Motywacją wprowadzenia tej zmiennej jest potrzeba ``uelastycznienia'' liniowych
ograniczeń (równanie~\ref{eq:hard_margin_class}) podczas analizowania nieliniowo rozdzielnych danych,
co pozwala na uzyskanie zbieżności algorytmu uczącego w obecności nieprawidłowych klasyfikacji
podczas stosowania odpowiedniej funkcji strat.

\begin{figure}[]
    \centering
    \includegraphics[width=0.3\textwidth]{images/soft-margin.drawio}
    \caption{ Przykład nieliniowo rozdzielnych próbek}
    \customsource
    \label{fig:sofm_margin}
\end{figure}

Po wprowadzeniu zmiennej $\xi$ do równiania~\ref{eq:hard_margin_class},
równanie to przybiera postać opisaną wzorami~\ref{eq:soft_margin}.

\begin{equation}
    \begin{aligned}
        w_0 +w^{T}x^{(i)} &\geq 1 - \xi^{(i)} && \text{ jeśli } y^{(i)} =1
        \\
        w_0 +w^{T}x^{(i)} &\leq -1 + \xi^{(i)} && \text{ jeśli } y^{(i)} =-1
        \\
        \text{dla } i&= 1\dots N \\
        \text{gdzie:} \\
        \xi & \text{ - wartość przesunięcia granicy przynależności}
    \end{aligned}
    \label{eq:soft_margin}
\end{equation}

Nowym celem optymalizacji staje wtedy się równanie~\ref{eq:hard_svm_min_problem}.
Za pomocą zmiennej $C$ kontroluje się wagę kary za niewłaściwą klasyfikację.
Duża wartość parametru $C$ odpowiada wysokim karom za błędy, z kolei przy niskich wartościach
kara nie będzie mocno wpływać na szerokość marginesu.
Dzięki temu parametrowi jest się w stanie regulować kompromis pomiędzy
obciążeniem a wariancją~\cite{raschka2017python} (rysunek~\ref{fig:wplyw_c_na_svm}).

\begin{equation}
    \begin{aligned}
        &\frac{1}{2}||w||^2 + C (\sum_{i}^{N} \xi^{(i)})\\
        \text{gdzie:} \\
        &C \text{ - mnożnik kary za złą klasyfikację}
    \end{aligned}
    \label{eq:hard_svm_min_problem}
\end{equation}

\begin{figure}[]
    \centering
    \includegraphics[width=0.7\textwidth]{images/wplyw_c_na_svm}
    \caption{  Wpływ zmiennej C na szerokość marginesu }
    \bibsource{\cite{raschka2017python}}
    \label{fig:wplyw_c_na_svm}
\end{figure}

\bigskip

W przypadku, gdy dostarczone dane nie są separowalne za pomocą hiperpłaszczyzny
w $N$ wymiarach (rysunek~\ref{fig:dane_nie_separowalne_liniowo})
wprowadza się dodatkowe wymiary za pomocą funkcji mapującej $\phi$.
Dodatkowe wymiary wprowadza się tak długo, aż dane staną się liniowo separowalne.
Przykładowo, w celu wyznaczenia granicy decyzyjności danych
przedstawionych na rysunku~\ref{fig:dane_nie_separowalne_liniowo},
funkcja mapująca, za pomocą której będzie możliwe wyznaczenie hiperpłaszczyzny,
została przedstawiona równaniem~\ref{eq:przykladowy_kernel}.
Rezultat mapowania danych z rysunku~\ref{fig:dane_nie_separowalne_liniowo} przez funkcję~\ref{eq:przykladowy_kernel}
został przedstawiony na rysunku~\ref{fig:wyniki_mapowan}.
Po wykonaniu mapowania oraz wyznaczeniu hiperpłaszczyzny dokonuje się rzutowania do pierwotnej przestrzeni cech ($\phi^{-1}$).

\begin{equation}
    \begin{aligned}
        \phi(x_1, x_2) = (z_1, z_2, z_3) = (x_1, x_2, x_1^2 + x_2^2)
    \end{aligned}
    \label{eq:przykladowy_kernel}
\end{equation}

\begin{figure}[]
    \centering
    \includegraphics[width=0.5\textwidth]{images/dane_nie_separowalne_liniowo}
    \caption{  Zestaw danych nie separowalnych liniowo }
    \bibsource{\cite{raschka2017python}}
    \label{fig:dane_nie_separowalne_liniowo}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=0.5\textwidth]{images/wyniki_mapowan}
    \caption{ Sposób wyznaczania nieliniowej granicy decyzyjności }
    \bibsource{\cite{raschka2017python}}
    \label{fig:wyniki_mapowan}
\end{figure}

\bigskip

Wszystkie opisane wyżej przypadki dotyczą problemów binarnych,
czyli takich gdzie dostarczona próbka należy do jednej z dwóch klas.
W związku z tym, że SVM obsługuje tylko klasyfikację binarną, dlatego w przypadku
gdy dana próbka może należeć do jeden z $N$ klas (klasyfikacja wieloklasowa)
dokonuje się rozbicia jednego problemu klasyfikacji wieloklasowej na wiele problemów klasyfikacji binarnej.
W tym celu stosuje się jedną z dwóch strategii - jeden kontra jeden (ang. one vs one --- OvO)
lub jeden przeciwko wszystkim (ang. one vs all --- OvA).
Obie te strategie zostały przedstawione na rysunku~\ref{fig:ovr} oraz~\ref{fig:ovo}.


\begin{figure}[]
    \centering
    \includegraphics[width=0.7\textwidth]{images/ovr}
    \caption{ Strategia jeden przeciwko wszystkim }
    \customsource
    \label{fig:ovr}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=0.7\textwidth]{images/ovo}
    \caption{ Strategia jeden kontra jeden }
    \customsource
    \label{fig:ovo}
\end{figure}



\section{Obsługa systemu}

Całość zaprojektowanego systemu ma być dostępna z przeglądarki internetowej.
Aby sprostać temu wymaganiu, należy napisać usługę, która jest w stanie obsługiwać połączenia
HTTP\footnote{Skrót z języka angielskiego od słów Hypertext Transfer Protocol}.
Z powodu, że narzędzia do wykrywania twarzy, obróbki zdjęć oraz FaceNet są udostępnione w języku python,
program zostanie również napisany w tym języku.
Oprócz samej strony internetowej potrzebna jest również baza danych,
w której będą przechowywane informacje o dostępnych zdjęciach.
Same zdjęcia będą przechowywane na serwerze plików.
Sporym ułatwieniem będzie również panel umożliwiający zarządzanie dostępnymi zdjęciami w systemie.

Narzędziem, które spełni wyżej postawione wymagania jest platforma programistyczna \textit{Django},
która umożliwia w łatwy sposób zarządzanie bazą danych za pomocą mapowania obiektowo-relacyjnego,
pozwala na zarządzanie danymi dostępnymi w bazie danych poprzez panel administratora, dostępny
z poziomu przeglądarki oraz co najważniejsze, jest napisana w języku python~\cite{djangodoc}.